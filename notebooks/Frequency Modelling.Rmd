---
title: "Modelling"
author: "Helitha Dharmadasa - z5451805"
date: "2024-10-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library("tidyverse")
library(scales)
library(MLmetrics)
library(ModelMetrics)
library(MetricsWeighted)
library(gridExtra)
library(caret)
library(lightgbm)
library(glmnet)
library(statmod)
library(mgcv)
library(splines)
library(Metrics)
library(shapviz)
library(kernelshap)
```


## Read Data

```{r}
frequency_df <- read_csv('../data/transformed/frequency.csv', col_names = TRUE)

prediction_data <- read_csv('../data/transformed/prediction.csv', col_names = TRUE)
```

```{r}
# Select the features we want
modelling_features <- c("claim_count", "pet_gender", "pet_de_sexed_age", "pet_is_switcher", "pet_age_months", "nb_contribution", "nb_excess", "nb_address_type_adj", "median_income", "nb_state", "owner_age_years", "nb_number_of_breeds", "nb_average_breed_size", "nb_breed_type", "nb_breed_trait", "is_multi_pet_plan", "quote_time_group", "earned_units", "mm_category")

# Select broad feature subset
modelling_df <- frequency_df %>%
  select(modelling_features)

prediction_df <- prediction_data %>%
  mutate(claim_count = NA,
         log_earned_units = log(earned_units)) %>%
  select(c(modelling_features, "log_earned_units"))

# Encode catrgorical vars as factors
categorical_features <- c("pet_gender", "pet_de_sexed_age", "pet_is_switcher", "nb_address_type_adj", "nb_state", "nb_breed_type", "nb_breed_trait","is_multi_pet_plan", "quote_time_group", "mm_category")

# Convert categorical features to factors
modelling_df <- modelling_df %>%
  mutate_at(vars(all_of(categorical_features)), as.factor)

# Convert unseen nb_breed_trait to unknown in predictions
prediction_df <- prediction_df %>%
  mutate(nb_breed_trait = if_else(nb_breed_trait %in% modelling_df$nb_breed_trait, nb_breed_trait, "unknown"))

prediction_df <- prediction_df %>%
  mutate_at(vars(all_of(categorical_features)), as.factor)

modelling_df <- modelling_df %>%
  mutate(log_earned_units = log(earned_units))

summary(modelling_df)

train_indices <- sample(1:nrow(modelling_df), size = 0.8 * nrow(modelling_df))
train_df <- modelling_df[train_indices, ]
test_df <- modelling_df[-train_indices, ]
```

# LightGBM
```{r, warning = FALSE, results = "hold", cache = TRUE, tidy.opts = list(width.cutoff = 70)}
# Light GBM needs data formated slightly differently, using provided lgb.Dataset.
train_lgbm_full <- lgb.Dataset(data = as.matrix(subset(modelling_df, select = -c(claim_count, earned_units, log_earned_units))),
                      label = as.matrix(subset(modelling_df, select = claim_count)),
                      init_score = as.matrix(subset(modelling_df, select = log_earned_units)),
                      categorical_feature = categorical_features)

# Custom metric function for LightGBM for poisson deviance
poisson_deviance_lgbm <- function(preds, dtrain){
  labels <- get_field(dtrain, "label")
  poisson_devaince_val <- deviance_poisson(predicted = preds, actual = labels)
  
  return(list(name="poisson_deviance",value=poisson_devaince_val,higher_better=FALSE))
}
```

```{r}
# Hyperparameter optimisation - SLOW, not recommended to run
# Optimal values are already used in next chunk

run_opt = FALSE

if (run_opt == TRUE) {
  param_grid <- expand.grid(
    num_leaves = c(31,64, 128),
    learning_rate = c(0.01, 0.05, 0.1),
    lambda_l1 = c(0, 0.1, 0.5),
    lambda_l2 = c(0, 0.1, 0.5),
    min_gain_to_split = c(0, 0.1)
  )
  
  best_score <- Inf
  best_params <- list()
  
  for (i in 1:nrow(param_grid)) {
    lgbm_params <- list(objective = "poisson",
                    boosting_type = "gbdt",
                    metric = "None",
                    nthread = 8,
                    num_leaves = param_grid$num_leaves[i],
                    learning_rate = param_grid$learning_rate[i],
                    lambda_l1 = param_grid$lambda_l1[i],
                    lambda_l2 = param_grid$lambda_l2[i],
                    min_gain_to_split = param_grid$min_gain_to_split[i]
                    ) 
    
    lgbm_cv <- lgb.cv(params = lgbm_params,
                      data = train_lgbm_full,
                      eval=poisson_deviance_lgbm,
                      nrounds = 500,
                      nfold = 5, 
                      verbose = -1) 
    
    min_score <- lgbm_cv$best_score
    print(min_score)
    
    # Update best parameters if current metric is lower
    if (min_score < best_score) {
      best_score <- min_score
      best_params <- lgbm_params
      best_cv <- lgbm_cv
    }
  }
}
```


```{r}
train_lgbm <- lgb.Dataset(data = as.matrix(subset(train_df, select = -c(claim_count, earned_units, log_earned_units))),
                      label = as.matrix(subset(train_df, select = claim_count)),
                      init_score = as.matrix(subset(train_df, select = log_earned_units)),
                      categorical_feature = categorical_features)

test_lgbm <- as.matrix(subset(test_df, select = -c(claim_count, earned_units, log_earned_units)))

# LGBM with optimised hyperparameters
lgbm_params <- list(objective = "poisson",
                  boosting_type = "gbdt",
                  metric = "None",
                  nthread = 8,
                  num_leaves = 128,
                  learning_rate = 0.1,
                  lambda_l1 = 0.5,
                  lambda_l2 = 0.5,
                  min_gain_to_split = 0.1
                  ) 
  
lgbm_cv <- lgb.cv(params = lgbm_params,
                  data = train_lgbm,
                  eval=poisson_deviance_lgbm,
                  nrounds = 500,
                  nfold = 10, 
                  verbose = -1) 

print(lgbm_cv$best_score)
```

```{r}
# Fit Light GBM with optimal rounds from CV.
lgbm_model <- lgb.train(params = lgbm_params,
                         data = train_lgbm,
                         nrounds = lgbm_cv$best_iter,
                         verbose = -1)
```

```{r}
# Out of sample error
lgbm_test_predictions <- predict(lgbm_model, test_lgbm)

lgbm_deviance <- deviance_poisson(predicted = lgbm_test_predictions, 
                                  actual = test_df$claim_count)

print(lgbm_deviance)
```

```{r}
shap_lgb_test <- shapviz(lgbm_model, X_pred = data.matrix(test_lgbm))  
```
```{r}
sv_importance(shap_lgb_test, kind = "bee")
```
```{r}
sv_importance(shap_lgb_test, show_numbers = TRUE)
```


#Frequency - Poisson Model

```{r}
poisson_deviance <- function(data, lev = NULL, model = NULL) {
  #pred <- ifelse(data$pred < 0, 0, data$pred)
  poisson_deviance_val <- deviance_poisson(predicted = data$pred, actual = data$obs)
  c(Poisson_Deviance = poisson_deviance_val)
}

# Stratify
cv_index <- createFolds(factor(ifelse(train_df$claim_count > 0, TRUE, FALSE)), 10, returnTrain = TRUE)

cv_ctrl <- trainControl(index = cv_index, 
                        method = "cv", 
                        number = 10,
                        summaryFunction = poisson_deviance,
                        savePredictions = TRUE,
                        allowParallel = TRUE)

#tune_grid <- expand.grid(alpha = 0, lambda = seq(0.001, 1, length = 10))
```


```{r}
poisson_cv <- train(claim_count ~ . -earned_units -log_earned_units + offset(log_earned_units), 
                               data = modelling_df, 
                               method = "glm", 
                               family = poisson(link = 'log'),
                               trControl = cv_ctrl,
                               metric = "Poisson_Deviance")

poisson_cv
summary(poisson_cv)


par(mfrow=c(2,2), mar=rep(4,4))
plot(poisson_cv$finalModel)
par(mfrow=c(1,1))
predictions <- poisson_cv$pred
plot_data <- predictions %>%
  pivot_longer(cols = c(obs, pred), names_to = "type", values_to = "value")
```
```{r}
poisson_glm <- glm(claim_count ~ . -earned_units -log_earned_units,
                   family = poisson(link = "log"),
                   offset = log_earned_units,
                   data = train_df)

summary(poisson_glm)
par(mfrow=c(2,2))
plot(poisson_glm)
par(mfrow=c(1,1))
```
```{r}
glm_test_predictions <- predict(poisson_glm, test_df, type = "response")

glm_deviance <- deviance_poisson(predicted = glm_test_predictions, actual = test_df$claim_count)

print(glm_deviance)
```
```{r}
feature_subset <- c("claim_count",
                    "nb_excess", 
                    "nb_average_breed_size", 
                    "owner_age_years", 
                    "pet_age_months",
                    "median_income",
                    "nb_contribution",
                    "mm_category",
                    "log_earned_units")

categorical_features_subset <- intersect(feature_subset, categorical_features)

subset_df <- modelling_df %>%
  select(feature_subset)

subset_prediction_df <- prediction_df %>%
  select(feature_subset)

# Light GBM needs data formated slightly differently, using provided lgb.Dataset.
subset_lgbm_full <- lgb.Dataset(data = as.matrix(subset(subset_df, select = -c(claim_count, log_earned_units))),
                      label = as.matrix(subset(subset_df, select = claim_count)),
                      init_score = as.matrix(subset(subset_df, select = log_earned_units)),
                      categorical_feature = categorical_features_subset)

subset_prediction_lgbm <- as.matrix(subset(subset_prediction_df, select = -c(claim_count, log_earned_units)))


```

```{r}
# Cross Validate Light GBM.
lgbm_cv <- lgb.cv(params = lgbm_params,
                  data = train_lgbm,
                  eval=poisson_deviance_lgbm,
                  nrounds = 500,
                  nfold = 10, 
                  verbose = -1) 

# Fit Light GBM with optimal rounds from CV.
final_model <- lgb.train(params = lgbm_params,
                         data = subset_lgbm_full,
                         nrounds = lgbm_cv$best_iter,
                         verbose = -1)

```

```{r}
# Convert unseen nb_breed_trait to unknown in predictions
#prediction_df <- prediction_df %>%
#  mutate(nb_breed_trait = if_else(nb_breed_trait %in% train_df$nb_breed_trait, nb_breed_trait, "unknown"))

final_predictions <- predict(final_model, subset_prediction_lgbm)

prediction_data <- prediction_data %>%
  mutate(predicted_claim_count = final_predictions)

write_csv(prediction_data, '../data/transformed/frequency_predictions.csv')
```

```{r}
shap_lgb_pred <- shapviz(final_model, X_pred = data.matrix(subset_prediction_lgbm))  

sv_waterfall(shap_lgb_pred, row_id = 1)
```
```{r}
sv_waterfall(shap_lgb_pred, row_id = 2)
```









