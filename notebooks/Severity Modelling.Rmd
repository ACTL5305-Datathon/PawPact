---
title: "Modelling"
author: "Helitha Dharmadasa - z5451805"
date: "2024-10-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pacman)
pacman::p_load(tidyverse,
               scales,
               MLmetrics,
               ModelMetrics,
               MetricsWeighted,
               gridExtra,
               caret,
               lightgbm,
               glmnet,
               statmod,
               car,
               shapviz,
               kernelshap,
	       MASS)
```

## Read Data

```{r}
severity_df <- read_csv("../data/transformed/severity.csv", col_names = TRUE)

prediction_data <- read_csv('../data/transformed/prediction.csv', col_names = TRUE)
```

```{r}
modelling_features <- c("claim_paid", 
                         "pet_gender", 
                         "pet_de_sexed_age", 
                         "pet_is_switcher", 
                         "pet_age_months", 
                         "nb_contribution", 
                         "nb_excess", 
                         "nb_address_type_adj", 
                         "median_income", 
                         "nb_state", 
                         "owner_age_years", 
                         "nb_number_of_breeds", 
                         "nb_average_breed_size", 
                         "nb_breed_type", 
                         "nb_breed_trait", 
                         "is_multi_pet_plan", 
                         "quote_time_group", 
                         "earned_units", 
                         "mm_category")
# Select the features we want
modelling_df <- severity_df %>% select(modelling_features)

prediction_df <- prediction_data %>%
  mutate(claim_paid = NA,
         log_earned_units = log(earned_units)) %>%
  select(c(modelling_features, "log_earned_units"))

# Encode categorical vars as factors
categorical_features <- c("pet_gender", 
                          "pet_de_sexed_age", 
                          "pet_is_switcher", 
                          "nb_address_type_adj", 
                          "nb_state", 
                          "nb_breed_type", 
                          "nb_breed_trait",
                          "is_multi_pet_plan", 
                          "quote_time_group", 
                          "mm_category")

# Convert categorical features to factors
modelling_df <- modelling_df %>%
  mutate_at(vars(all_of(categorical_features)), as.factor)

prediction_df <- prediction_df %>%
  mutate_at(vars(all_of(categorical_features)), as.factor)

modelling_df <- modelling_df %>%
  mutate(log_earned_units = log(earned_units))

summary(modelling_df)

train_indices <- sample(1:nrow(modelling_df), size = 0.8 * nrow(modelling_df))
train_df <- modelling_df[train_indices, ]
test_df <- modelling_df[-train_indices, ]
```

```{r}
# Step 2: Filter the tibble to exclude claim_paid values above the 99th percentile
#train_df <- train_df %>%
#  filter(claim_paid <= quantile(train_df$claim_paid, 0.999))

hist(train_df$claim_paid,
     #xlim=c(0,2000),
     breaks=150,
     xlab="Paid Claims",
     main="")

```

# LIGHT GBM
```{r, warning = FALSE, results = "hold", cache = TRUE, tidy.opts = list(width.cutoff = 70)}
# Light GBM needs data formated slightly differently, using provided lgb.Dataset.
train_lgbm_full <- lgb.Dataset(data = as.matrix(subset(modelling_df, select = -c(claim_paid, earned_units, log_earned_units))),
                      label = as.matrix(subset(modelling_df, select = claim_paid)),
                      init_score = as.matrix(subset(modelling_df, select = log_earned_units)),
                      categorical_feature = categorical_features)

# Custom metric function for LightGBM for poisson deviance
gamma_deviance_lgbm <- function(preds,dtrain){
  labels <- get_field(dtrain, "label")
  gamma_deviance_val <- deviance_gamma(predicted = preds, actual = labels)
  
  return(list(name="gamma_deviance",value=gamma_deviance_val,higher_better=FALSE))
}
```

```{r}
# Hyperparameter optimisation - SLOW, not recommended to run
# Optimal values are already used in next chunk
run_opt = FALSE

if (run_opt == TRUE) {
  param_grid <- expand.grid(
    num_leaves = c(8, 15, 31),
    learning_rate = c(0.01, 0.1, 0.2),
    lambda_l1 = c(0, 0.05, 0.1),
    lambda_l2 = c(0, 0.05, 0.1),
    min_gain_to_split = c(0, 0.1),
    min_data_in_leaf = c(3,10,20)
  )
  
  best_score <- Inf
  best_params <- list()
  
  for (i in 1:nrow(param_grid)) {
    lgbm_params <- list(objective = "gamma",
                    boosting_type = "gbdt",
                    metric = "None",
                    nthread = 8,
                    num_leaves = param_grid$num_leaves[i],
                    learning_rate = param_grid$learning_rate[i],
                    lambda_l1 = param_grid$lambda_l1[i],
                    lambda_l2 = param_grid$lambda_l2[i],
                    min_data_in_leaf = param_grid$min_data_in_leaf[i],
                    min_gain_to_split = param_grid$min_gain_to_split[i]
                    ) 
    
    lgbm_cv <- lgb.cv(params = lgbm_params,
                      data = train_lgbm,
                      eval=gamma_deviance_lgbm,
                      nrounds = 500,
                      nfold = 5, #use less folds for faster processing
                      verbose = -1) 
    
    min_score <- lgbm_cv$best_score
    print(min_score)
    
    # Update best parameters if current metric is lower
    if (min_score < best_score) {
      best_score <- min_score
      best_params <- lgbm_params
      best_cv <- lgbm_cv
    }
  }
}

```

```{r}
train_lgbm <- lgb.Dataset(data = as.matrix(subset(train_df, select = -c(claim_paid, earned_units, log_earned_units))),
                      label = as.matrix(subset(train_df, select = claim_paid)),
                      init_score = as.matrix(subset(train_df, select = log_earned_units)),
                      categorical_feature = categorical_features)

test_lgbm <- as.matrix(subset(test_df, select = -c(claim_paid, earned_units, log_earned_units)))

# LGBM with optimised hyperparameters
lgbm_params <- list(objective = "gamma",
                  boosting_type = "gbdt",
                  metric = "None",
                  nthread = 8,
                  num_leaves = 8,
                  learning_rate = 0.1,
                  min_data_in_leaf = 10,
                  min_gain_to_split = 0.1) 


# Cross Validate Light GBM with Early Stopping.
lgbm_cv <- lgb.cv(params = lgbm_params,
                    eval=gamma_deviance_lgbm,
                    data = train_lgbm,
                    nrounds = 900,
                    nfold = 10, 
                    verbose = -1) 


print(lgbm_cv$best_score)
```

```{r}
# Fit Light GBM with optimal rounds from CV.
lgbm_model <- lgb.train(params = lgbm_params,
                         data = train_lgbm,
                         nrounds = lgbm_cv$best_iter,
                         verbose = -1)
```

```{r}
# Out of sample error
lgbm_test_predictions <- predict(lgbm_model, test_lgbm)

lgbm_deviance <- deviance_gamma(predicted = lgbm_test_predictions, 
                                  actual = test_df$claim_paid)

print(lgbm_deviance)
```

```{r}
shap_lgb_test <- shapviz(lgbm_model, X_pred = data.matrix(test_lgbm))  
```

```{r}
sv_importance(shap_lgb_test, kind = "bee")
```
```{r}
sv_importance(shap_lgb_test, show_numbers = TRUE)
```

```{r}
gamma_deviance <- function(data, lev = NULL, model = NULL) {
  gamma_deviance_val <- deviance_gamma(predicted = data$pred, actual = data$obs)
  c(Gamma_Deviance = gamma_deviance_val)
}

cv_ctrl <- trainControl(method = "cv", 
                        number = 10,
                        summaryFunction = gamma_deviance,
                        savePredictions = TRUE,
                        allowParallel = TRUE)
```



```{r}
gamma_cv <- train(claim_paid ~ . -earned_units + offset(log_earned_units), 
                               data = train_df, 
                               method = "glm", 
                               family = Gamma(link = "log"),
                               trControl = cv_ctrl,
                               metric = "Gamma_Deviance")
summary(gamma_cv)

#outlierTest(gamma_cv$finalModel)
boxcox(gamma_cv$finalModel, lambda = seq(-2, 2, 0.1))


#vif(model$finalModel)

#cooks_dist <- cooks.distance(model$finalModel)
#plot(cooks_dist, type = "h")

par(mfrow=c(2,2), mar=rep(4,4))
plot(gamma_cv$finalModel)
par(mfrow=c(1,1))

predictions <- gamma_cv$pred
plot_data <- predictions %>%
  pivot_longer(cols = c(obs, pred), names_to = "type", values_to = "value")
```

```{r}
gamma_glm <- glm(claim_paid ~ . -earned_units -log_earned_units,
                   family = Gamma(link = "log"),
                   offset = log_earned_units,
                   data = train_df)
summary(gamma_glm)
par(mfrow=c(2,2))
plot(gamma_glm)
par(mfrow=c(1,1))
```

```{r}
final_model <- gamma_glm

# Convert unseen nb_breed_trait to unknown in predictions
prediction_df <- prediction_df %>%
  mutate(nb_breed_trait = if_else(nb_breed_trait %in% train_df$nb_breed_trait, nb_breed_trait, "unknown"))

final_predictions <- predict(final_model, prediction_df, type = "response")

prediction_data <- prediction_data %>%
  mutate(predicted_claim_paid = final_predictions)

write_csv(prediction_data, '../data/transformed/severity_predictions.csv')
```

```{r}
ggplot(plot_data, aes(x = value, fill = type)) +
  geom_histogram(position = "identity", bins = 30, alpha = 0.9) +
  scale_fill_manual(values = c("obs" = "red", "pred" = "green")) +
  labs(x = "Value", y = "Frequency", 
       title = "Overlapping Histograms of Predicted vs Actual Values") +
  theme_minimal()
```

